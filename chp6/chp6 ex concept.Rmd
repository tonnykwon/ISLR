---
title: "chp6 ex concept"
author: "kwon"
date: "2018년 8월 19일"
output: html_document
---

# We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1
models, containing 0, 1, 2, . . . , p predictors. Explain your answers: \
##(a) Which of the three models with k predictors has the smallest
training RSS? \
The model with p+1 predictors.

##(b) Which of the three models with k predictors has the smallest
test RSS?\
best subset selection may have the smalelst test RSS, since it considers all possible combinations.

##(c) True or False:
i. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by forward stepwise selection. TRUE
\

ii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by backward stepwise selection.
TRUE
\

iii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by forward stepwise selection.
FALSE
\

iv. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by backward stepwise selection.
FALSE
\

v. The predictors in the k-variable model identified by best
subset are a subset of the predictors in the (k + 1)-variable
model identified by best subset selection.
FALSE

# 2 
For parts (a) through (c), indicate which of i. through iv. is correct.
Justify your answer.
##(a) The lasso, relative to least squares, is:
###i. More flexible and hence will give improved prediction accuracy
when its increase in bias is less than its decrease in
variance. \

### ii. More flexible and hence will give improved prediction accuracy
when its increase in variance is less than its decrease
in bias.\

### iii. Less flexible and hence will give improved prediction accuracy
when its increase in bias is less than its decrease in
variance.
\
Correct, with penalty, it decreases flexibility with penalty.

### iv. Less flexible and hence will give improved prediction accuracy
when its increase in variance is less than its decrease
in bias.

(b) Repeat (a) for ridge regression relative to least squares. iii
(c) Repeat (a) for non-linear methods relative to least squares. ii

# 3 Suppose we estimate the regression coefficients in a linear regression
model by minimizing ~(Least sqares with L1 Penalty)
for a particular value of s. For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

##(a) As we increase s from 0, the training RSS will:
### i. Increase initially, and then eventually start decreasing in an inverted U shape.

### ii. Decrease initially, and then eventually start increasing in a U shape.
### iii. Steadily increase.
### iv. Steadily decrease.
\
As we increase s from 0, all beta will increase and training error will decrease steadily.

### v. Remain constant.


(b) Repeat (a) for test RSS. ii
(c) Repeat (a) for variance. iii
(d) Repeat (a) for (squared) bias. iv
(e) Repeat (a) for the irreducible error. v

# 4 L2 Penalty, increase lambda from 0,
## (a) iii
## (b) ii
## (c) iv
## (d) iii
## (e) v

# 5
## (a) 
Minimize: 
$$\sum\limits_{i=1}^n {(y_i - \hat{\beta}0 - \sum\limits_{j=1}^p {\hat{\beta}jx_j} )^2} + \lambda \sum\limits_{i=1}^p \hat{\beta}_i^2$$

In this case, $\hat{\beta}_0 = 0$ and $n = p = 2$. So, the optimization looks like:

Minimize: 
$$(y_1 - \hat{\beta}_1x_{11} - \hat{\beta}_2x_{12})^2 + (y_2 - \hat{\beta}_1x_{21} - \hat{\beta}_2x_{22})^2 + \lambda (\hat{\beta}_1^2 + \hat{\beta}_2^2)$$

##(b)

Now we are given that, $x_{11} = x_{12} = x_1$ and $x_{21} = x_{22} = x_2$. We take derivatives of above expression with respect to both $\hat{\beta_1}$ and $\hat{\beta_2}$ and setting them equal to zero find that, 
$$\hat\beta_1 = \frac{x_1y_1 +x_2y_2 - \hat{\beta_2(x_1^2+x_2^2)}}{\lambda+x_1^2+x_2^2}$$
and 
$$\hat{\beta}_2 = \frac{x_1y_1 + x_2y_2 - \hat{\beta}_1(x_1^2 + x_2^2)}{\lambda + x_1^2 + x_2^2}$$

Symmetry in these expressions suggests that $\hat{\beta}_1 = \hat{\beta}_2$


## (c)
Minimize: 
$$\sum\limits_{i=1}^n {(y_i - \hat{\beta}0 - \sum\limits_{j=1}^p {\hat{\beta}jx_j} )^2} + \lambda \sum\limits_{i=1}^p |\hat{\beta}_i|$$

Minimize: 
$$(y_1 - \hat{\beta}_1x_{11} - \hat{\beta}_2x_{12})^2 + (y_2 - \hat{\beta}_1x_{21} - \hat{\beta}_2x_{22})^2 + \lambda |(\hat{\beta}_1^2| + |\hat{\beta}_2)|$$

## (d)
Here is a geometric interpretation of the solutions for the equation in c above. We use the alternate form of Lasso constraints $| \hat{\beta}_1 | + | \hat{\beta}_2 | < s$.

The Lasso constraint take the form $| \hat{\beta}1 | + | \hat{\beta}2 | < s$, which when plotted take the familiar shape of a diamond centered at origin $(0, 0)$. Next consider the squared optimization constraint $(y_1 - \hat{\beta}1x{11} - \hat{\beta}2x{12})^2 + (y_2 - \hat{\beta}1x{21} - \hat{\beta}2x{22})^2$. We use the facts $x{11} = x{12}$, $x_{21} = x_{22}$, $x_{11} + x_{21} = 0$, $x_{12} + x_{22} = 0$ and $y_1 + y_2 = 0$ to simplify it to

Minimize: $2.(y_1 - (\hat{\beta}_1 + \hat{\beta}2)x{11})^2$.

This optimization problem has a simple solution: $\hat{\beta}_1 + \hat{\beta}2 = \frac{y_1}{x{11}}$. This is a line parallel to the edge of Lasso-diamond $\hat{\beta}_1 + \hat{\beta}_2 = s$. Now solutions to the original Lasso optimization problem are contours of the function $(y_1 - (\hat{\beta}_1 + \hat{\beta}2)x{11})^2$ that touch the Lasso-diamond $\hat{\beta}_1 + \hat{\beta}_2 = s$. Finally, as $\hat{\beta}_1$ and $\hat{\beta}_2$ very along the line $\hat{\beta}_1 + \hat{\beta}2 = \frac{y_1}{x{11}}$, these contours touch the Lasso-diamond edge $\hat{\beta}_1 + \hat{\beta}_2 = s$ at different points. As a result, the entire edge $\hat{\beta}_1 + \hat{\beta}_2 = s$ is a potential solution to the Lasso optimization problem!

Similar argument can be made for the opposite Lasso-diamond edge: $\hat{\beta}_1 + \hat{\beta}_2 = -s$.

Thus, the Lasso problem does not have a unique solution. The general form of solution is given by two line segments:

$\hat{\beta}_1 + \hat{\beta}_2 = s; \hat{\beta}_1 \geq 0; \hat{\beta}_2 \geq 0$ and $\hat{\beta}_1 + \hat{\beta}_2 = -s; \hat{\beta}_1 \leq 0; \hat{\beta}_2 \leq 0$


#6
$$(y - \beta)^2 + \lambda\beta^2$$
```{r}
y=2
lambda=2
betas = seq(-10, 10, 0.1)
func = (y-betas)^2 +lambda*betas^2
plot(betas, func, pch=20)
est.beta = y / (1+lambda)
est.func = (y - est.beta)^2 + lambda * est.beta^2
points(est.beta, est.func, col="red", pch=4, lwd=5, cex=est.beta)
```
##(b)
$$  (y-\beta)^2 + \lambda| \beta\ | $$
```{r}
func = (y-betas)^2 + lambda*abs(betas)
plot(betas, func)
if(y > lambda/2){
  est.beta = y-lambda/2
} else if(y< -lambda/2){
  est.beta = y+lambda/2
} else{
  est.beta = 0
}
est.func = (y - est.beta)^2 + lambda * est.beta^2
points(est.beta, est.func, col="red", pch=4, lwd=5, cex=est.beta)
```
