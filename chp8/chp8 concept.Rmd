---
title: "chp8 concept"
author: "kwon"
date: "2018년 8월 24일"
output: html_document
---

#1
```{r}
library(ISLR)
library(tree)
boston.tree = tree(nox~., data=Boston)
plot(boston.tree)
text(boston.tree, pretty=0)

plot(nox~indus, data=Boston)
lines(x=c(0,0), y=c(0,16))
```

#2
Based on 8.2, the boosting function is:
$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$

On first iteration, the boosting algorithm creates stump on one predictor, as it is stump. And to minimize the residuals, other distinct precitors will be used for creating stumps

j) a) $$\hat{f}^j(x) = \beta_{1_j} I(X_j < t_j) + \beta_{0_j}$$

j) b) $$\hat{f}(x) = \lambda\hat{f}^1(X_1) + \dots + \hat{f}^j(X_j) + \dots + \hat{f}^{p-1}(X_{p-1}) + \hat{f}^p(X_p)$$

$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$

#3
```{r}
pmk = seq(0,1, 0.001)
gini = pmk*(1-pmk)*2
entropy = -(pmk*log(pmk)+(1-pmk)*log(1-pmk))
class.e = 1-pmax(pmk, 1-pmk)
matplot(pmk, cbind(gini, entropy,class.e), col=c("red", "blue", "green"))

```

#4
##a























